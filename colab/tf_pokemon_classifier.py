# -*- coding: utf-8 -*-
"""tf-vehicle-classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O875YSUGDqrM3WQsvzbby0ez_6RszUJV
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
import shutil
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import random
import string

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)
  
# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

batch_size = 32
img_height = 100
img_width = 100

os.chdir("/content/drive/MyDrive/vehicle-100x100")

dirs = os.listdir()
# count = 0
for dir in dirs:
  # if count==5:
  #   break
  vehicle_name = dir.split("-")[0]
  # print(vehicle_name) 
  if not os.path.exists("/content/drive/MyDrive/vehicle-100x100-clubbed/"+vehicle_name):
    os.makedirs("/content/drive/MyDrive/vehicle-100x100-clubbed/"+vehicle_name)
  shutil.move("/content/drive/MyDrive/vehicle-100x100/"+dir,"/content/drive/MyDrive/vehicle-100x100-clubbed/"+vehicle_name+"/"+dir)
  # shutil.copy("/content/drive/MyDrive/vehicle-100x100/"+dir,"/content/drive/MyDrive/vehicle-100x100-clubbed/"+vehicle_name+"/"+dir)
  # count +=1
    # os.makedirs("/content/drive/MyDrive/vehicle-100x100-clubbed"+vehicle_name)

directory = "/content/drive/MyDrive/vehicle-100x100-clubbed"
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  directory,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  directory,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)
print(len(class_names))

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

num_classes = len(class_names)

data_augmentation = keras.Sequential(
  [
    layers.experimental.preprocessing.RandomFlip("horizontal", 
                                                 input_shape=(img_height, 
                                                              img_width,
                                                              3)),
    layers.experimental.preprocessing.RandomRotation(0.1),
    layers.experimental.preprocessing.RandomZoom(0.1),
  ]
)

model = Sequential([
  # data_augmentation,
  layers.experimental.preprocessing.Rescaling(1./255
                                              , input_shape=(img_height, img_width, 3)
                                              ),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  # layers.BatchNormalization(),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 7,padding='same', activation='relu'),
  # layers.BatchNormalization(),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 5, activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(128, 3, activation='relu'),
  layers.BatchNormalization(),
  layers.Conv2D(64, 7, activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(16, 1,padding='same', activation='relu'),
  # layers.BatchNormalization(),
  # layers.MaxPooling2D(),
  # layers.Conv2D(64, 3,padding='same', activation='relu'),
  # layers.MaxPooling2D(),
  # layers.Dropout(0.2),
  layers.Flatten(),
  # layers.Dropout(0.2),
  layers.Dense(128, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(num_classes,activation="softmax")
])


model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

epochs=80
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
)

model.save('/content/drive/MyDrive/saved-models/vehicle')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()




vehicle_url = "https://static3.depositphotos.com/1003789/172/i/600/depositphotos_1726217-stock-photo-wrecked-car-1.jpg"


print("downloading ",vehicle_url)
vehicle_path = tf.keras.utils.get_file(''.join(random.choice(string.ascii_lowercase) for i in range(10)), origin=vehicle_url)

img = keras.preprocessing.image.load_img(
    vehicle_path, target_size=(img_height, img_width)
)

img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)